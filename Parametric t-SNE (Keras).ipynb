{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently:\n",
    "* training the original network (500-500-2000-2) on mnist with tsne loss\n",
    "* training the original network on a pre-embedded set of 2d points with mse loss\n",
    "* training a convolution net on a pre-embedded set of 2d points with mse loss\n",
    "\n",
    "Next:\n",
    "* check tsne loss on test set for all three models\n",
    "* if we lower the learning rate in response to test set loss can we acheive lower final loss?\n",
    "* what architecture allows us to minimize the number of parameters while also minimizing test tsne loss?\n",
    "* does dropout have an effect on the generalization of any of the models?\n",
    "* can we \"deep dream\" a point in 2d space to recover an input that produces it?\n",
    "* what happens if we permute the input data but use the same joint probabilities?\n",
    "* what if we instead use P from an unsupervised mid-dimensional representation like an autoencoder or VAE? (e.g., 48D)\n",
    "* what if we use t-SNE loss as a contributing cost function to an autoencoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.objectives import categorical_crossentropy\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 5000\n",
    "\n",
    "%time (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Hbeta(D, beta):\n",
    "    P = np.exp(-D * beta)\n",
    "    sumP = np.sum(P)\n",
    "    H = np.log(sumP) + beta * np.sum(np.multiply(D, P)) / sumP\n",
    "    P = P / sumP\n",
    "    return H, P\n",
    "\n",
    "def x2p(X, u=15, tol=1e-4, print_iter=500, max_tries=50, verbose=0):\n",
    "    # Initialize some variables\n",
    "    n = X.shape[0]                     # number of instances\n",
    "    P = np.zeros((n, n))               # empty probability matrix\n",
    "    beta = np.ones(n)                  # empty precision vector\n",
    "    logU = np.log(u)                   # log of perplexity (= entropy)\n",
    "    \n",
    "    # Compute pairwise distances\n",
    "    if verbose > 0: print('Computing pairwise distances...')\n",
    "    sum_X = np.sum(np.square(X), axis=1)\n",
    "    # note: translating sum_X' from matlab to numpy means using reshape to add a dimension\n",
    "    D = sum_X + sum_X[:,None] + -2 * X.dot(X.T)\n",
    "\n",
    "    # Run over all datapoints\n",
    "    if verbose > 0: print('Computing P-values...')\n",
    "    for i in range(n):\n",
    "        \n",
    "        if verbose > 1 and print_iter and i % print_iter == 0:\n",
    "            print('Computed P-values {} of {} datapoints...'.format(i, n))\n",
    "        \n",
    "        # Set minimum and maximum values for precision\n",
    "        betamin = float('-inf')\n",
    "        betamax = float('+inf')\n",
    "        \n",
    "        # Compute the Gaussian kernel and entropy for the current precision\n",
    "        indices = np.concatenate((np.arange(0, i), np.arange(i + 1, n)))\n",
    "        Di = D[i, indices]\n",
    "        H, thisP = Hbeta(Di, beta[i])\n",
    "        \n",
    "        # Evaluate whether the perplexity is within tolerance\n",
    "        Hdiff = H - logU\n",
    "        tries = 0\n",
    "        while abs(Hdiff) > tol and tries < max_tries:\n",
    "            \n",
    "            # If not, increase or decrease precision\n",
    "            if Hdiff > 0:\n",
    "                betamin = beta[i]\n",
    "                if np.isinf(betamax):\n",
    "                    beta[i] *= 2\n",
    "                else:\n",
    "                    beta[i] = (beta[i] + betamax) / 2\n",
    "            else:\n",
    "                betamax = beta[i]\n",
    "                if np.isinf(betamin):\n",
    "                    beta[i] /= 2\n",
    "                else:\n",
    "                    beta[i] = (beta[i] + betamin) / 2\n",
    "            \n",
    "            # Recompute the values\n",
    "            H, thisP = Hbeta(Di, beta[i])\n",
    "            Hdiff = H - logU\n",
    "            tries += 1\n",
    "        \n",
    "        # Set the final row of P\n",
    "        P[i, indices] = thisP\n",
    "        \n",
    "    if verbose > 0: \n",
    "        print('Mean value of sigma: {}'.format(np.mean(np.sqrt(1 / beta))))\n",
    "        print('Minimum value of sigma: {}'.format(np.min(np.sqrt(1 / beta))))\n",
    "        print('Maximum value of sigma: {}'.format(np.max(np.sqrt(1 / beta))))\n",
    "    \n",
    "    return P, beta\n",
    "\n",
    "def compute_joint_probabilities(samples, batch_size=5000, d=2, perplexity=30, tol=1e-5, verbose=0):\n",
    "    v = d - 1\n",
    "    \n",
    "    # Initialize some variables\n",
    "    n = samples.shape[0]\n",
    "    batch_size = min(batch_size, n)\n",
    "    \n",
    "    # Precompute joint probabilities for all batches\n",
    "    if verbose > 0: print('Precomputing P-values...')\n",
    "    batch_count = int(n / batch_size)\n",
    "    P = np.zeros((batch_count, batch_size, batch_size))\n",
    "    for i, start in enumerate(range(0, n - batch_size + 1, batch_size)):   \n",
    "        curX = samples[start:start+batch_size]                   # select batch\n",
    "        P[i], beta = x2p(curX, perplexity, tol, verbose=verbose) # compute affinities using fixed perplexity\n",
    "        P[i][np.isnan(P[i])] = 0                                 # make sure we don't have NaN's\n",
    "        P[i] = (P[i] + P[i].T) # / 2                             # make symmetric\n",
    "        P[i] = P[i] / P[i].sum()                                 # obtain estimation of joint probabilities\n",
    "        P[i] = np.maximum(P[i], np.finfo(P[i].dtype).eps)\n",
    "\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time P = compute_joint_probabilities(X_train, batch_size=batch_size, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time np.save('P.npy', P)\n",
    "# %time P = np.load('P.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# P is the joint probabilities for this batch (Keras loss functions call this y_true)\n",
    "# activations is the low-dimensional output (Keras loss functions call this y_pred)\n",
    "def tsne(P, activations):\n",
    "#     d = K.shape(activations)[1]\n",
    "    d = 2 # TODO: should set this automatically, but the above is very slow for some reason\n",
    "    n = batch_size # TODO: should set this automatically\n",
    "    v = d - 1.\n",
    "    eps = K.variable(10e-15) # needs to be at least 10e-8 to get anything after Q /= K.sum(Q)\n",
    "    sum_act = K.sum(K.square(activations), axis=1)\n",
    "    Q = K.reshape(sum_act, [-1, 1]) + -2 * K.dot(activations, K.transpose(activations))\n",
    "    Q = (sum_act + Q) / v\n",
    "    Q = K.pow(1 + Q, -(v + 1) / 2)\n",
    "    Q *= K.variable(1 - np.eye(n))\n",
    "    Q /= K.sum(Q)\n",
    "    Q = K.maximum(Q, eps)\n",
    "    C = K.log((P + eps) / (Q + eps))\n",
    "    C = K.sum(P * C)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(500, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(2000, activation='relu'))\n",
    "model.add(Dense(2))\n",
    "sgd = SGD(lr=0.1)\n",
    "%time model.compile(loss=tsne, optimizer=sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train = P.reshape(X_train.shape[0], -1)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time model.fit(X_train, Y_train, batch_size=batch_size, shuffle=False, nb_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_model(embedding, labels):\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    plt.scatter(embedding[:,0], embedding[:,1], marker='o', s=1, edgecolor='', c=labels)\n",
    "    fig.tight_layout()\n",
    "# plot_model(model.predict(data), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tsne import bh_sne\n",
    "%time X_2d = bh_sne(X_train.astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %time np.save('X_2d.npy', X_2d)\n",
    "# %time X_2d = np.load('X_2d.npy')\n",
    "X_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# around 1.2M parameters\n",
    "encoder = Sequential()\n",
    "encoder.add(Dense(500, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "encoder.add(Dense(500, activation='relu'))\n",
    "encoder.add(Dense(2000, activation='relu'))\n",
    "encoder.add(Dense(2))\n",
    "%time encoder.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time encoder.fit(X_train, X_2d, nb_epoch=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_model(transfer, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "def plot_differences(embedding, actual, lim=1000):\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.gca()\n",
    "    for a, b in zip(embedding, actual)[:lim]:\n",
    "        ax.add_line(Line2D((a[0], b[0]), (a[1], b[1]), linewidth=1))\n",
    "    ax.autoscale_view()\n",
    "    plt.show()\n",
    "# plot_differences(model.predict(data), X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# around 600K parameters\n",
    "nb_filters = 32\n",
    "nb_pool = 2\n",
    "nb_conv = 3\n",
    "conv = Sequential()\n",
    "conv.add(Convolution2D(nb_filters, nb_conv, nb_conv, input_shape=(1, 28, 28)))\n",
    "conv.add(Activation('relu'))\n",
    "conv.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n",
    "conv.add(Activation('relu'))\n",
    "conv.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "conv.add(Dropout(0.25))\n",
    "conv.add(Flatten())\n",
    "conv.add(Dense(128))\n",
    "conv.add(Activation('relu'))\n",
    "conv.add(Dense(2))\n",
    "%time conv.compile(loss='mean_squared_error', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_conv = X_train.reshape(-1, 1, 28, 28)\n",
    "%time conv.fit(X_train_conv, X_2d, nb_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_model(conv, X_train_conv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_differences(conv, X_train_conv, X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# around 1.2M parameters\n",
    "decoder = Sequential()\n",
    "decoder.add(Dense(2000, activation='relu', input_shape=(2,)))\n",
    "decoder.add(Dense(500, activation='relu'))\n",
    "decoder.add(Dense(500, activation='relu'))\n",
    "decoder.add(Dense(X_train.shape[1]))\n",
    "%time decoder.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time decoder.fit(X_2d, X_train, nb_epoch=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_mosaic(images_flat, n):\n",
    "    nx = ny = n\n",
    "    w = h = int(np.sqrt(len(images_flat[0])))\n",
    "    images = images_flat.reshape(-1, h, w)\n",
    "    image_gen = iter(images)\n",
    "    mosaic = np.empty((h*ny, w*nx))\n",
    "    for i in range(ny):\n",
    "        for j in range(nx):\n",
    "            mosaic[(nx-i-1)*w:(nx-i)*w, j*h:(j+1)*h] = next(image_gen)\n",
    "    return mosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cStringIO import StringIO\n",
    "import PIL.Image\n",
    "import IPython.display\n",
    "import shutil\n",
    "def showarray(a, fmt='png', filename=None):\n",
    "    a = np.uint8(np.clip(a, 0, 255))\n",
    "    image_data = StringIO()\n",
    "    PIL.Image.fromarray(a).save(image_data, fmt)\n",
    "    IPython.display.display(IPython.display.Image(data=image_data.getvalue()))\n",
    "    if filename is not None:\n",
    "        with open(filename, 'w') as f:\n",
    "            image_data.seek(0)\n",
    "            shutil.copyfileobj(image_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = X_train.shape[1]\n",
    "ae = Sequential()\n",
    "ae.add(Dense(500, activation='relu', weights=encoder.layers[0].get_weights(), input_shape=(n,)))\n",
    "ae.add(Dense(500, activation='relu', weights=encoder.layers[1].get_weights()))\n",
    "ae.add(Dense(2000, activation='relu', weights=encoder.layers[2].get_weights()))\n",
    "ae.add(Dense(2, weights=encoder.layers[3].get_weights()))\n",
    "ae.add(Dense(2000, activation='relu', weights=decoder.layers[0].get_weights()))\n",
    "ae.add(Dense(500, activation='relu', weights=decoder.layers[1].get_weights()))\n",
    "ae.add(Dense(500, activation='relu', weights=decoder.layers[2].get_weights()))\n",
    "ae.add(Dense(n, weights=decoder.layers[3].get_weights()))\n",
    "%time ae.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time ae.fit(X_train, X_train, nb_epoch=100, verbose=2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoded = ae.predict(X_train)\n",
    "mosaic = showarray(255 * make_mosaic(decoded, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "def get_submodel(model, start, end):\n",
    "    return theano.function([model.layers[start].input],\n",
    "                          model.layers[end].get_output(train=False),\n",
    "                          allow_input_downcast=True)\n",
    "def get_encoder(ae):\n",
    "    return get_submodel(ae, 0, (len(ae.layers) / 2) - 1)\n",
    "def get_decoder(ae):\n",
    "    return get_submodel(ae, len(ae.layers) / 2, (len(ae.layers) / 2) - 1) # this doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ae_encoder = get_encoder(ae)\n",
    "# ae_decoder = get_decoder(ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoded = encoder.predict(X_train)\n",
    "ae_encoded = ae_encoder(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_differences(encoded, ae_encoded, lim=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_model(encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_model(ae_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ae_decoder = Sequential()\n",
    "ae_decoder.add(Dense(2000, activation='relu', weights=ae.layers[4].get_weights(), input_shape=(2,)))\n",
    "ae_decoder.add(Dense(500, activation='relu', weights=ae.layers[5].get_weights()))\n",
    "ae_decoder.add(Dense(500, activation='relu', weights=ae.layers[6].get_weights()))\n",
    "ae_decoder.add(Dense(X_train.shape[1], weights=ae.layers[7].get_weights()))\n",
    "%time ae_decoder.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_grid(model, example_data, filename=None, n=128):\n",
    "    x = np.linspace(example_data[:,0].min(), example_data[:,0].max(), n)\n",
    "    y = np.linspace(example_data[:,1].min(), example_data[:,1].max(), n)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    samples_encoded = np.array(zip(xv.flatten(), yv.flatten()))\n",
    "    samples_decoded = model.predict(samples_encoded)\n",
    "    showarray(255 * make_mosaic(samples_decoded, n), filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_grid(decoder, encoded, filename='output-orig.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_grid(ae_decoder, ae_encoded, filename='output-finer.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = X_train.shape[1]\n",
    "ae_scratch = Sequential()\n",
    "ae_scratch.add(Dense(500, activation='relu', input_shape=(n,)))\n",
    "ae_scratch.add(Dense(500, activation='relu'))\n",
    "ae_scratch.add(Dense(2000, activation='relu'))\n",
    "ae_scratch.add(Dense(2))\n",
    "ae_scratch.add(Dense(2000, activation='relu'))\n",
    "ae_scratch.add(Dense(500, activation='relu'))\n",
    "ae_scratch.add(Dense(500, activation='relu'))\n",
    "ae_scratch.add(Dense(n))\n",
    "%time ae_scratch.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ae_scratch.fit(X_train, X_train, nb_epoch=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ae_scratch_encoder = get_encoder(ae_scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ae_scratch_encoded = ae_scratch_encoder(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ae_scratch_decoder = Sequential()\n",
    "ae_scratch_decoder.add(Dense(2000, activation='relu', weights=ae_scratch.layers[4].get_weights(), input_shape=(2,)))\n",
    "ae_scratch_decoder.add(Dense(500, activation='relu', weights=ae_scratch.layers[5].get_weights()))\n",
    "ae_scratch_decoder.add(Dense(500, activation='relu', weights=ae_scratch.layers[6].get_weights()))\n",
    "ae_scratch_decoder.add(Dense(X_train.shape[1], weights=ae_scratch.layers[7].get_weights()))\n",
    "%time ae_scratch_decoder.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_grid(ae_scratch_decoder, ae_scratch_encoded, filename='output-scratch.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
